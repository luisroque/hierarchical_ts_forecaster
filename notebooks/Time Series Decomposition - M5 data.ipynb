{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-05-20T16:33:37.781447Z",
     "iopub.status.busy": "2021-05-20T16:33:37.781052Z",
     "iopub.status.idle": "2021-05-20T16:33:38.589651Z",
     "shell.execute_reply": "2021-05-20T16:33:38.588697Z",
     "shell.execute_reply.started": "2021-05-20T16:33:37.781351Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from itertools import cycle\n",
    "pd.set_option('max_columns', 50)\n",
    "plt.style.use('bmh')\n",
    "color_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "color_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "INPUT_DIR = 'm5-data'\n",
    "cal = pd.read_csv(f'{INPUT_DIR}/calendar.csv')\n",
    "stv = pd.read_csv(f'{INPUT_DIR}/sales_train_validation.csv')\n",
    "ss = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')\n",
    "sellp = pd.read_csv(f'{INPUT_DIR}/sell_prices.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given historic sales data in the `sales_train_validation` dataset.\n",
    "- rows exist in this dataset for days d_1 to d_1913. We are given the department, category, state, and store id of the item.\n",
    "- d_1914 - d_1941 represents the `validation` rows which we will predict in stage 1\n",
    "- d_1942 - d_1969 represents the `evaluation` rows which we will predict for the final competition standings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T16:33:51.622844Z",
     "iopub.status.busy": "2021-05-20T16:33:51.622452Z",
     "iopub.status.idle": "2021-05-20T16:33:51.673052Z",
     "shell.execute_reply": "2021-05-20T16:33:51.672064Z",
     "shell.execute_reply.started": "2021-05-20T16:33:51.622815Z"
    }
   },
   "outputs": [],
   "source": [
    "stv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the data with real dates\n",
    "- We are given a calendar with additional information about past and future dates.\n",
    "- The calendar data can be merged with our days data\n",
    "- From this we can find weekly and annual trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T16:33:51.675191Z",
     "iopub.status.busy": "2021-05-20T16:33:51.674769Z",
     "iopub.status.idle": "2021-05-20T16:33:51.6916Z",
     "shell.execute_reply": "2021-05-20T16:33:51.690845Z",
     "shell.execute_reply.started": "2021-05-20T16:33:51.675148Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calendar data looks like this (only showing columns we care about for now)\n",
    "cal[['d','date','event_name_1','event_name_2',\n",
    "     'event_type_1','event_type_2', 'snap_CA']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T16:33:51.693313Z",
     "iopub.status.busy": "2021-05-20T16:33:51.692715Z",
     "iopub.status.idle": "2021-05-20T16:33:51.962867Z",
     "shell.execute_reply": "2021-05-20T16:33:51.9614Z",
     "shell.execute_reply.started": "2021-05-20T16:33:51.693273Z"
    }
   },
   "outputs": [],
   "source": [
    "d_cols = [c for c in stv.columns if 'd_' in c] # sales data columns\n",
    "\n",
    "# Below we are chaining the following steps in pandas:\n",
    "# 1. Select the item.\n",
    "# 2. Set the id as the index, Keep only sales data columns\n",
    "# 3. Transform so it's a column\n",
    "# 4. Plot the data\n",
    "stv.loc[stv['id'] == 'FOODS_3_090_CA_3_validation'] \\\n",
    "    .set_index('id')[d_cols] \\\n",
    "    .T \\\n",
    "    .plot(figsize=(15, 5),\n",
    "          title='FOODS_3_090_CA_3 sales by \"d\" number',\n",
    "          color=next(color_cycle))\n",
    "plt.legend('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T16:33:51.964867Z",
     "iopub.status.busy": "2021-05-20T16:33:51.964461Z",
     "iopub.status.idle": "2021-05-20T16:33:52.19868Z",
     "shell.execute_reply": "2021-05-20T16:33:52.197669Z",
     "shell.execute_reply.started": "2021-05-20T16:33:51.964823Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge calendar on our items' data\n",
    "example = stv.loc[stv['id'] == 'FOODS_3_090_CA_3_validation'][d_cols].T\n",
    "example = example.rename(columns={8412:'FOODS_3_090_CA_3'}) # Name it correctly\n",
    "example = example.reset_index().rename(columns={'index': 'd'}) # make the index \"d\"\n",
    "example = example.merge(cal, how='left', validate='1:1')\n",
    "example.set_index('date')['FOODS_3_090_CA_3'] \\\n",
    "    .plot(figsize=(15, 5),\n",
    "          color=next(color_cycle),\n",
    "          title='FOODS_3_090_CA_3 sales by actual sale dates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T16:33:52.206257Z",
     "iopub.status.busy": "2021-05-20T16:33:52.205854Z",
     "iopub.status.idle": "2021-05-20T16:33:52.213702Z",
     "shell.execute_reply": "2021-05-20T16:33:52.212718Z",
     "shell.execute_reply.started": "2021-05-20T16:33:52.206195Z"
    }
   },
   "outputs": [],
   "source": [
    "stv.columns[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T16:33:52.215756Z",
     "iopub.status.busy": "2021-05-20T16:33:52.215259Z",
     "iopub.status.idle": "2021-05-20T16:33:52.37431Z",
     "shell.execute_reply": "2021-05-20T16:33:52.373249Z",
     "shell.execute_reply.started": "2021-05-20T16:33:52.215713Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Number of unique items: {np.unique(stv.item_id).shape[0]}')\n",
    "print(f'Number of unique department: {np.unique(stv.dept_id).shape[0]}')\n",
    "print(f'Number of unique categories: {np.unique(stv.cat_id).shape[0]}')\n",
    "print(f'Number of unique stores: {np.unique(stv.store_id).shape[0]}')\n",
    "print(f'Number of unique states: {np.unique(stv.state_id).shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T16:33:52.376476Z",
     "iopub.status.busy": "2021-05-20T16:33:52.376043Z",
     "iopub.status.idle": "2021-05-20T16:34:06.491076Z",
     "shell.execute_reply": "2021-05-20T16:34:06.490172Z",
     "shell.execute_reply.started": "2021-05-20T16:33:52.376434Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transform column wide days to single column \n",
    "\n",
    "stv = stv.melt(list(stv.columns[:6]), var_name='day', value_vars=list(stv.columns[6:]), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T16:34:06.492717Z",
     "iopub.status.busy": "2021-05-20T16:34:06.492439Z",
     "iopub.status.idle": "2021-05-20T16:34:43.396554Z",
     "shell.execute_reply": "2021-05-20T16:34:43.395463Z",
     "shell.execute_reply.started": "2021-05-20T16:34:06.492682Z"
    }
   },
   "outputs": [],
   "source": [
    "# Group by the groups to consider (remove product_id as there are 3049 unique) \n",
    "\n",
    "stv = stv.groupby(['dept_id', 'cat_id', 'store_id', 'state_id', 'day']).sum('value').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T16:34:43.399863Z",
     "iopub.status.busy": "2021-05-20T16:34:43.399276Z",
     "iopub.status.idle": "2021-05-20T16:34:43.418248Z",
     "shell.execute_reply": "2021-05-20T16:34:43.417461Z",
     "shell.execute_reply.started": "2021-05-20T16:34:43.399818Z"
    }
   },
   "outputs": [],
   "source": [
    "days_calendar = np.concatenate((stv['day'].unique().reshape(-1,1), cal['date'][:-56].unique().reshape(-1,1)), axis=1)\n",
    "df_caldays = pd.DataFrame(days_calendar, columns = ['day','Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T16:34:43.420169Z",
     "iopub.status.busy": "2021-05-20T16:34:43.41957Z",
     "iopub.status.idle": "2021-05-20T16:34:43.470379Z",
     "shell.execute_reply": "2021-05-20T16:34:43.469538Z",
     "shell.execute_reply.started": "2021-05-20T16:34:43.420124Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add calendar days\n",
    "\n",
    "stv = stv.merge(df_caldays, how='left', on='day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T16:34:43.472132Z",
     "iopub.status.busy": "2021-05-20T16:34:43.471561Z",
     "iopub.status.idle": "2021-05-20T16:34:43.52098Z",
     "shell.execute_reply": "2021-05-20T16:34:43.520194Z",
     "shell.execute_reply.started": "2021-05-20T16:34:43.472092Z"
    }
   },
   "outputs": [],
   "source": [
    "stv['Date'] = stv['Date'].astype('datetime64[ns]')\n",
    "stv.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T16:34:43.522565Z",
     "iopub.status.busy": "2021-05-20T16:34:43.522113Z",
     "iopub.status.idle": "2021-05-20T16:34:45.225076Z",
     "shell.execute_reply": "2021-05-20T16:34:45.224131Z",
     "shell.execute_reply.started": "2021-05-20T16:34:43.522521Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transform in weekly data\n",
    "\n",
    "stv_weekly = stv.groupby(['dept_id', 'cat_id', 'store_id', 'state_id']).resample('W', on='Date')['value'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T16:34:45.226642Z",
     "iopub.status.busy": "2021-05-20T16:34:45.22635Z",
     "iopub.status.idle": "2021-05-20T16:34:45.239691Z",
     "shell.execute_reply": "2021-05-20T16:34:45.238826Z",
     "shell.execute_reply.started": "2021-05-20T16:34:45.226613Z"
    }
   },
   "outputs": [],
   "source": [
    "stv_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T16:34:45.241536Z",
     "iopub.status.busy": "2021-05-20T16:34:45.241106Z",
     "iopub.status.idle": "2021-05-20T16:34:45.274836Z",
     "shell.execute_reply": "2021-05-20T16:34:45.273821Z",
     "shell.execute_reply.started": "2021-05-20T16:34:45.241501Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the structure to then apply the grouping transformation\n",
    "\n",
    "stv_pivot = stv_weekly.reset_index().pivot(index='Date',columns=['dept_id', 'cat_id', 'store_id', 'state_id'], values='value')\n",
    "stv_pivot = stv_pivot.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stv_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first series\n",
    "\n",
    "series_1 = stv_pivot.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics import tsaplots as smplots\n",
    "plt.rc(\"figure\", figsize=(20,15))\n",
    "plt.rc(\"font\", size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition = sm.tsa.seasonal_decompose(series_1, model='additive')\n",
    "fig = decomposition.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Decomposition\n",
    "\n",
    "### Trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s begin the step by step decomposition of this time series.\n",
    "\n",
    "STEP 1: Try to guess the duration of the seasonal component in your data. In the above example, we’ll guess it to be 52 weeks.\n",
    "\n",
    "STEP 2: Now run a 52 week centered moving average on the data. This moving average is spread across a total of 53 weeks. i.e. 26 weeks each on the left and right side of the center week. The 52 week centered MA is an average of two moving averages that are shifted from each other by 1 week, effectively making it a weighted moving average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = series_1.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add an empty column to store the 2x12 centered MA values\n",
    "df1['trend'] = np.nan#Fill it up with the 2x12 centered MA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns = [' '.join(col).strip() for col in df1.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns = ['sales', 'trend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_ = df1.copy()\n",
    "for i in range(26, df1.shape[0]-26):\n",
    "    total_i = (\n",
    "        # Sum the values from the first and last week weighted \n",
    "        df1_['sales'].iloc[i - 26] * 1.0 / (52*2) + df1_['sales'].iloc[i + 26] * 1.0 / (52*2))\n",
    "    for j in range(-25, 26):\n",
    "        # Add the remaining values weighted by 1/52 (they belong to both MA, so they have double of the weight)\n",
    "        total_i += df1_['sales'].iloc[i+j]/52\n",
    "    df1['trend'].iloc[i] = np.round(total_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming first value (2011-07-31)\n",
    "\n",
    "(sum(df1['sales'][0:52])/52 + sum(df1['sales'][1:53])/52)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df1_t['trend']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 3: Now we have a decision to make. Depending on whether the composition is multiplicative or additive, we’ll need to divide or subtract the trend component from the original time series to retrieve the seasonal and noise components. If we inspect the original sales time series, we can see that the seasonal swings are not increasing in proportion to the current value of the time series. Hence we’ll assume that the seasonality is additive. We’ll also take a small leap of faith to assume that the noise is additive.\n",
    "\n",
    "Thus the retail used car sales time series is assumed to have the following multiplicative decomposition model:\n",
    "\n",
    "Time series value = trend component + seasonal component + noise component\n",
    "\n",
    "Therefore:\n",
    "\n",
    "seasonal component + noise component = Time series value - trend component\n",
    "\n",
    "We’ll add a new column into our data frame and fill it with the sum of the seasonal and noise components using the above formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['seasonality&noise'] = df1['sales']-df1['trend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df1['seasonality&noise'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 4: Next, we will get the ‘pure’ seasonal component out of the mixture of seasonality and noise, by calculating the average value of the seasonal component for all first weeks of January, all second weeks of February and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['week'] = df1.index.isocalendar().week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['seasonality'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_ = df1.copy()\n",
    "for i in df1['week'].unique():\n",
    "    df1['seasonality'].iloc[i] = np.sum(df1_.loc[df1_['week']==i,'seasonality&noise'])/df1_[df1_['week']==i].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_.loc[df1_['week']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df1['seasonality']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
